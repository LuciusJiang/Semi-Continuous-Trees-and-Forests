{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semi-Continuous Trees (with Synthetic Data Simulation Results).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWNxopCLCbYn"
      },
      "source": [
        "import math\n",
        "import timeit\n",
        "import random\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbBO6Ru0DGZ6"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo3NXXQ9DO97"
      },
      "source": [
        "## X generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOrX2vr9DR9Q"
      },
      "source": [
        "def X_generator(n = 10000, p = 20):\n",
        "  \n",
        "  df = pd.DataFrame(np.random.randint(0, 1, size=(n, p)))\n",
        "\n",
        "  for col_index in range(p):\n",
        "    if col_index % 2 != 0:\n",
        "      df[col_index] = np.random.binomial(1, 0.5, n)\n",
        "    else:\n",
        "      df[col_index] = np.random.normal(0, 1, n)\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmVHrVJMDTNy"
      },
      "source": [
        "## W generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkgmdiKDXJh"
      },
      "source": [
        "def W_generator(X):\n",
        "\n",
        "  n, p = X.shape\n",
        "\n",
        "  beta_0 = -3.0\n",
        "  eta = pd.Series(np.zeros(n) + beta_0)\n",
        "\n",
        "  for col_index in range(p):\n",
        "    eta += X[col_index] * 1.0 / np.sqrt(col_index + 1)\n",
        "  ZISC_propensity_score = np.exp(eta) / (1 + np.exp(eta))\n",
        "  \n",
        "  W = pd.Series(np.zeros(n)) * 1.0\n",
        "  for i in range(n):\n",
        "    W[i] = np.random.binomial(1, ZISC_propensity_score[i], 1)\n",
        "  \n",
        "  return W"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6RcdrAfDcOO"
      },
      "source": [
        "## DGP: Data Generation Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKonfp2SDg_p"
      },
      "source": [
        "def f_1(X):\n",
        "  return (X[0] > 1).astype(float)\n",
        "\n",
        "def f_2(X):\n",
        "  return X[0]\n",
        "\n",
        "def f_3(X):\n",
        "  return X[0] + X[2] + X[4] + X[6] + X[7] + X[8] - 2.0\n",
        "\n",
        "def f_4(X):\n",
        "  return 1.0 * (X[0] > 1).astype(float) * (X[2] > 0).astype(float) + 2.0 * (X[4] > 1).astype(float) * (X[6] > 0).astype(float) + 3.0 * X[7] * X[8]\n",
        "\n",
        "def f_5(X):\n",
        "  return 1.0 * (~(X[1] != X[3])).astype(float) + 3.0 * (~(X[3] != X[5])).astype(float) + 5.0 * (~(X[5] != X[1])).astype(float)\n",
        "\n",
        "def f_6(X):\n",
        "  return X[0] **2 + X[1] + X[2] ** 2 + X[3] + X[4] ** 2 +  X[5] +  X[6] ** 2 + X[7] + X[8] ** 2\n",
        "\n",
        "def f_7(X):\n",
        "  return 0.5 * f_3(X) + 0.5 * f_5(X)\n",
        "\n",
        "def f_8(X):\n",
        "  return np.sin(math.pi * X[0] * X[1]) + 2 * X[2] * X[3] + 0.5 * X[4] * X[5]\n",
        "\n",
        "DGP = {\n",
        "    \"f_1\": f_1,\n",
        "    \"f_2\": f_2,\n",
        "    \"f_3\": f_3,\n",
        "    \"f_4\": f_4,\n",
        "    \"f_5\": f_5,\n",
        "    \"f_6\": f_6,\n",
        "    \"f_7\": f_7,\n",
        "    \"f_8\": f_8\n",
        "}\n",
        "\n",
        "Tree_Optimal_Depth  = {\n",
        "    \"f_1\": 8,\n",
        "    \"f_2\": 8,\n",
        "    \"f_3\": 6,\n",
        "    \"f_4\": 4,\n",
        "    \"f_5\": 7,\n",
        "    \"f_6\": 6,\n",
        "    \"f_7\": 6,\n",
        "    \"f_8\": 6\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxcrowq_DjFp"
      },
      "source": [
        "## Training Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34-u0Oc2DnOi"
      },
      "source": [
        "def training_data(f = \"f_1\", n = 10000, p = 20):\n",
        "\n",
        "  X = X_generator(n, p)\n",
        "  W = W_generator(X)\n",
        "  f_X = DGP[f](X)\n",
        "  \n",
        "  # Standardlized f_x\n",
        "  f_X_standardized = (f_X - f_X.mean())/(f_X.std())\n",
        "  # true z, which is the quantity we would like to estimate\n",
        "  z = np.exp(f_X_standardized + 0.5)\n",
        "  \n",
        "  # generate observation y\n",
        "  y = pd.Series(np.zeros(n)) * 1.0\n",
        "  for i in range(n):\n",
        "    y[i] = np.exp(np.random.normal(f_X_standardized[i], 1)) * W[i]\n",
        "\n",
        "  # generate y_tilde = y / prepensity_score\n",
        "  # prepensity_score estiamtion (propensity_score_hat) based on Logistic Regression\n",
        "  logisticRegr = LogisticRegression()\n",
        "  logisticRegr.fit(X, W)\n",
        "  propensity_score_hat = logisticRegr.predict_proba(X)[:,1]\n",
        "  y_tilde = y / propensity_score_hat\n",
        "  \n",
        "  return X.to_numpy(), z.to_numpy(), y.to_numpy(), y_tilde.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFClNRWg0bdh"
      },
      "source": [
        "# SSL: Self-Training Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD02DQChpGXU"
      },
      "source": [
        "## Self-Training Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxlDEUQH0gar"
      },
      "source": [
        "def semi_self_training_LR(X_train, y_train, X_test):\n",
        "    \n",
        "  Regressor_with_labeled_data_only = LinearRegression()\n",
        "  Regressor_with_labeled_data_only.fit(X_train[y_train>0], y_train[y_train>0])\n",
        "  y_train_unlabel_pred = Regressor_with_labeled_data_only.predict(X_train[y_train==0])\n",
        "\n",
        "  X_train_new = np.concatenate((X_train[y_train>0], X_train[y_train==0]), axis=0)\n",
        "  y_train_new = np.concatenate((y_train[y_train>0], y_train_unlabel_pred), axis=0)\n",
        "\n",
        "  Regressor_with_pred_labeled_data = LinearRegression()\n",
        "  Regressor_with_pred_labeled_data.fit(X_train_new, y_train_new)\n",
        "  \n",
        "  return Regressor_with_pred_labeled_data.predict(X_test)\n",
        "\n",
        "def semi_self_training_Tree(X_train, y_train, X_test, max_depth):\n",
        "    \n",
        "  Regressor_with_labeled_data_only = DecisionTreeRegressor(max_depth=max_depth)\n",
        "  Regressor_with_labeled_data_only.fit(X_train[y_train>0], y_train[y_train>0])\n",
        "  y_train_unlabel_pred = Regressor_with_labeled_data_only.predict(X_train[y_train==0])\n",
        "\n",
        "  X_train_new = np.concatenate((X_train[y_train>0], X_train[y_train==0]), axis=0)\n",
        "  y_train_new = np.concatenate((y_train[y_train>0], y_train_unlabel_pred), axis=0)\n",
        "\n",
        "  Regressor_with_pred_labeled_data = DecisionTreeRegressor(max_depth=max_depth)\n",
        "  Regressor_with_pred_labeled_data.fit(X_train_new, y_train_new)\n",
        "  \n",
        "  return Regressor_with_pred_labeled_data.predict(X_test)\n",
        "\n",
        "def semi_self_training_Forest(X_train, y_train, X_test, max_depth):\n",
        "    \n",
        "  Regressor_with_labeled_data_only = RandomForestRegressor(n_estimators=100, max_features='sqrt', max_depth=max_depth)\n",
        "  Regressor_with_labeled_data_only.fit(X_train[y_train>0], y_train[y_train>0])\n",
        "  y_train_unlabel_pred = Regressor_with_labeled_data_only.predict(X_train[y_train==0])\n",
        "\n",
        "  X_train_new = np.concatenate((X_train[y_train>0], X_train[y_train==0]), axis=0)\n",
        "  y_train_new = np.concatenate((y_train[y_train>0], y_train_unlabel_pred), axis=0)\n",
        "\n",
        "  Regressor_with_pred_labeled_data = RandomForestRegressor(n_estimators=100, max_features='sqrt', max_depth=max_depth)\n",
        "  Regressor_with_pred_labeled_data.fit(X_train_new, y_train_new)\n",
        "  \n",
        "  return Regressor_with_pred_labeled_data.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QryGvoyvpCvL"
      },
      "source": [
        "## Semi-Supervised Tree and Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu89JbyW1pMf"
      },
      "source": [
        "### Tree Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSlUM0ItpO-I"
      },
      "source": [
        "class Semi_Supervised_Tree(object):\n",
        "    def __init__(self, weight = 0.5, splitter='best', max_depth = 10, min_num_of_pos = 2, min_num_of_zero = 1):\n",
        "        self.depth = 0\n",
        "        self.weight = weight\n",
        "        self.max_depth = max_depth\n",
        "        self.splitter = splitter\n",
        "        self.min_num_of_pos = min_num_of_pos\n",
        "        self.min_num_of_zero = min_num_of_zero\n",
        "        self.trees = {}\n",
        "\n",
        "    def fit(self, X, y, par_node = {}, depth = 0):\n",
        "        if (depth == 0):\n",
        "          self.X_variance = np.var(X, axis=0)\n",
        "          self.y_plus_variance = np.var(y[y>0])\n",
        "\n",
        "        n_0, n_plus, y_plus_mean, y_plus_var = self.basic_summary_statistics(y)\n",
        "        col_index, cutoff, cost = self.find_best_split_of_all(X, y)\n",
        "        \n",
        "        if par_node is None:  # base case 1: tree stops at previous level\n",
        "            return None\n",
        "        elif (n_0 <= self.min_num_of_pos or n_plus <= self.min_num_of_zero): # base case 2: 100 zero or 50 positive instances in one node    \n",
        "            return {'val_CME': y_plus_mean,  \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        elif depth >= self.max_depth: # base case 3: max depth reached \n",
        "            return {'val_CME': y_plus_mean, \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        elif (cutoff == None): #base case 4: cannot split based one our rules\n",
        "            return {'val_CME': y_plus_mean, \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        else: \n",
        "            y_left = y[X[:, col_index] < cutoff]\n",
        "            y_right = y[X[:, col_index] >= cutoff]\n",
        "            par_node = {'col_index': col_index,\n",
        "                        'cutoff':cutoff,\n",
        "                        'val_CME': y_plus_mean,\n",
        "                        'n_0': n_0,\n",
        "                        'n_plus': n_plus,\n",
        "                        'n': n_0 + n_plus,\n",
        "                        'depth': depth}\n",
        "            par_node['left'] = self.fit(X[X[:, col_index] < cutoff], y_left, {}, depth+1)\n",
        "            par_node['right'] = self.fit(X[X[:, col_index] >= cutoff], y_right, {}, depth+1)\n",
        "            self.depth += 1 \n",
        "            self.trees = par_node\n",
        "            return par_node\n",
        "\n",
        "    def basic_summary_statistics(self, y):\n",
        "        \"\"\"\n",
        "        Return n_0, n_+, y_+_mean, y_+_var\n",
        "        \"\"\"\n",
        "        y_0 = y[y == 0]\n",
        "        y_plus = y[y > 0]\n",
        "        n_0 = len(y_0)\n",
        "        n_plus = len(y_plus)\n",
        "        y_plus_mean = np.mean(y_plus)\n",
        "        y_plus_var = np.var(y_plus)\n",
        "        \n",
        "        return n_0, n_plus, y_plus_mean, y_plus_var\n",
        "    \n",
        "    def find_best_split_of_all(self, X, y):\n",
        "        col_index = None\n",
        "        min_cost = float(\"inf\")\n",
        "        cutoff = None\n",
        "\n",
        "        num_features = len(X.T)\n",
        "        if self.splitter == 'sqrt':\n",
        "          num_features_for_split = int(np.sqrt(num_features))\n",
        "        else:\n",
        "          num_features_for_split = int(num_features)\n",
        "\n",
        "        random_indices = np.random.choice(num_features, size=num_features_for_split, replace=False)\n",
        "\n",
        "        for i, col in enumerate(X.T):\n",
        "          if (i in random_indices):\n",
        "            #print(\"col index: \" + str(i))\n",
        "            cur_cost, cur_cutoff = self.find_best_split(col, X, y)\n",
        "            if cur_cost <= min_cost:\n",
        "                min_cost = cur_cost\n",
        "                col_index = i\n",
        "                cutoff = cur_cutoff\n",
        "        #print(\"local best col: \" + str(col))\n",
        "        #print(\"local best cutoff: \" + str(cutoff))\n",
        "        #print(\"local best min_cost: \" + str(min_cost))\n",
        "        return col_index, cutoff, min_cost\n",
        "\n",
        "    def find_best_split(self, col, X, y):\n",
        "        min_cost = float(\"inf\")\n",
        "        cutoff = None\n",
        "\n",
        "        n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L = 0, 0, 0, 0\n",
        "        n_0_R = sum(y == 0)\n",
        "        n_plus_R = sum(y != 0)\n",
        "        y_plus_sum_R = sum(y)\n",
        "        y_plus_squared_sum_R = sum(y ** 2)\n",
        "\n",
        "        X_sum_L = np.zeros(X.shape[1])\n",
        "        X_squared_sum_L = np.zeros(X.shape[1])\n",
        "        X_sum_R = X.sum(axis=0)\n",
        "        X_squared_sum_R = np.square(X).sum(axis=0)\n",
        "\n",
        "        sorted_col_index = np.argsort(col)\n",
        "        sorted_X, sorted_y, sorted_col = X[sorted_col_index], y[sorted_col_index], col[sorted_col_index]\n",
        "\n",
        "        for i in range(len(col)):\n",
        "            if i == 0:\n",
        "              continue\n",
        "            else:\n",
        "              to_be_moved_X = sorted_X[i - 1]\n",
        "              X_sum_L += to_be_moved_X\n",
        "              X_squared_sum_L += np.square(to_be_moved_X)\n",
        "              X_sum_R -= to_be_moved_X\n",
        "              X_squared_sum_R -= np.square(to_be_moved_X)\n",
        "              \n",
        "              to_be_moved_y = sorted_y[i - 1]              \n",
        "              if (to_be_moved_y == 0):\n",
        "                n_0_L += 1\n",
        "                n_0_R -= 1\n",
        "              else:\n",
        "                n_plus_L += 1\n",
        "                n_plus_R -= 1 \n",
        "                y_plus_sum_L += to_be_moved_y\n",
        "                y_plus_squared_sum_L += to_be_moved_y ** 2\n",
        "                y_plus_sum_R -= to_be_moved_y\n",
        "                y_plus_squared_sum_R -= to_be_moved_y ** 2             \n",
        "            if (n_0_L < 1 or n_plus_L < 2 or n_0_R < 1 or n_plus_R < 2):\n",
        "              continue\n",
        "            if sorted_col[i-1] == sorted_col[i]:\n",
        "              continue\n",
        "            current_cost = self.ssl_cost_of_one_node(X_sum_L, X_squared_sum_L, n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L) + self.ssl_cost_of_one_node(X_sum_R, X_squared_sum_R, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R)\n",
        "            #print(\"cut off value: \" + str(sorted_col[i]))\n",
        "            #print(\"current cost: \" + str(current_cost))\n",
        "            if current_cost <= min_cost:\n",
        "                min_cost = current_cost\n",
        "                cutoff = sorted_col[i]\n",
        "        return min_cost, cutoff\n",
        "\n",
        "    def ssl_cost_of_one_node(self, X_sum, X_squared_sum, n_0, n_plus, y_plus_sum, y_plus_squared_sum):\n",
        "        n = n_0 + n_plus\n",
        "        cost_y_num = y_plus_squared_sum / n_plus -  (y_plus_sum / n_plus)**2\n",
        "        cost_y = cost_y_num / self.y_plus_variance\n",
        "\n",
        "        cost_X = 0\n",
        "        cost_X_num = X_squared_sum / n - np.square(X_sum / n)\n",
        "        p = len(cost_X_num)\n",
        "        for i in range(p):\n",
        "          cost_X += cost_X_num[i] / self.X_variance[i]\n",
        "\n",
        "        return n * (self.weight * cost_y + (1 - self.weight) / p * cost_X)\n",
        "\n",
        "    def predict_CME(self, X):\n",
        "        results = np.array([0.0]*len(X))\n",
        "        for i, c in enumerate(X):\n",
        "            results[i] = self._get_prediction_CME(c)\n",
        "        return results\n",
        "    \n",
        "    def _get_prediction_CME(self, row):\n",
        "        cur_layer = self.trees\n",
        "        while cur_layer.get('cutoff'):\n",
        "            if row[cur_layer['col_index']] < cur_layer['cutoff']:\n",
        "                cur_layer = cur_layer['left']\n",
        "            else:\n",
        "                cur_layer = cur_layer['right']\n",
        "        else:\n",
        "            return cur_layer.get('val_CME')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjSc-3Nz10Se"
      },
      "source": [
        "### Forest Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0wDTTGq13rH"
      },
      "source": [
        "def SSRF(X_train, y_train, X_test, \n",
        "         weight = 0.5, splitter='sqrt', max_depth=10, min_num_of_pos=2, min_num_of_zero=1, num_trees=100):\n",
        "  \n",
        "  SSRF_predicted = np.zeros(X_test.shape[0])\n",
        "\n",
        "  for tree_index in range(0, num_trees):\n",
        "    Semi_Supervised_Tree_ = Semi_Supervised_Tree(weight=weight, splitter=splitter, max_depth=max_depth, min_num_of_pos=min_num_of_pos, min_num_of_zero=min_num_of_zero)\n",
        "    Semi_Supervised_Tree_.fit(X_train, y_train)\n",
        "    SSRF_predicted += Semi_Supervised_Tree_.predict_CME(X_test)\n",
        "\n",
        "  return SSRF_predicted / num_trees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htaDN3-TCc4J"
      },
      "source": [
        "# Semi-Continuous Tree and Random Forest\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3rs4HDwC8qa"
      },
      "source": [
        "## Tree Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8EwqxhqC_uh"
      },
      "source": [
        "class Semi_Continuous_Tree(object):\n",
        "    def __init__(self, splitting_criteria = \"SSE\", splitter='best', max_depth = 10, min_num_of_pos = 2, min_num_of_zero = 1):\n",
        "        self.depth = 0\n",
        "        self.splitting_criteria = splitting_criteria\n",
        "        self.max_depth = max_depth\n",
        "        self.splitter = splitter\n",
        "        self.min_num_of_pos = min_num_of_pos\n",
        "        self.min_num_of_zero = min_num_of_zero\n",
        "        self.trees = {}\n",
        "\n",
        "    def fit(self, X, y, y_tilde, y_for_split, par_node = {}, depth = 0):\n",
        "        n_0, n_plus, y_plus_mean, y_plus_var = self.basic_summary_statistics(y)\n",
        "        col, cutoff, cost = self.find_best_split_of_all(X, y_for_split)\n",
        "        if par_node is None:  # base case 1: tree stops at previous level\n",
        "            return None\n",
        "        elif (n_0 <= self.min_num_of_pos or n_plus <= self.min_num_of_zero): # base case 2: 100 zero or 50 positive instances in one node    \n",
        "            return {'val_CME': y_plus_mean, \n",
        "                    'val_TOE': np.mean(y_tilde), \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        elif depth >= self.max_depth: # base case 3: max depth reached \n",
        "            return {'val_CME': y_plus_mean, \n",
        "                    'val_TOE': np.mean(y_tilde), \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        elif (cutoff == None): #base case 4: cannot split based one our rules\n",
        "            return {'val_CME': y_plus_mean, \n",
        "                    'val_TOE': np.mean(y_tilde), \n",
        "                    'n_0': n_0,\n",
        "                    'n_plus': n_plus,\n",
        "                    'n': n_0 + n_plus,\n",
        "                    'depth': depth}\n",
        "        else: \n",
        "            y_left = y[X[:, col] < cutoff]\n",
        "            y_right = y[X[:, col] >= cutoff]\n",
        "            y_tilde_left = y_tilde[X[:, col] < cutoff]\n",
        "            y_tilde_right = y_tilde[X[:, col] >= cutoff]\n",
        "            y_for_split_left = y_for_split[X[:, col] < cutoff]\n",
        "            y_for_split_right = y_for_split[X[:, col] >= cutoff]\n",
        "            par_node = {'col_index': col,\n",
        "                        'cutoff':cutoff,\n",
        "                        'val_CME': y_plus_mean,\n",
        "                        'val_TOE': np.mean(y_tilde),\n",
        "                        'n_0': n_0,\n",
        "                        'n_plus': n_plus,\n",
        "                        'n': n_0 + n_plus,\n",
        "                        'depth': depth}\n",
        "            par_node['left'] = self.fit(X[X[:, col] < cutoff], y_left, y_tilde_left, y_for_split_left, {}, depth+1)\n",
        "            par_node['right'] = self.fit(X[X[:, col] >= cutoff], y_right, y_tilde_right, y_for_split_right, {}, depth+1)\n",
        "            self.depth += 1 \n",
        "            self.trees = par_node\n",
        "            return par_node\n",
        "    \n",
        "    def find_best_split_of_all(self, X, y):\n",
        "        col = None\n",
        "        min_cost = float(\"inf\")\n",
        "        cutoff = None\n",
        "\n",
        "        num_features = len(X.T)\n",
        "        if self.splitter == 'sqrt':\n",
        "          num_features_for_split = int(np.sqrt(num_features))\n",
        "        else:\n",
        "          num_features_for_split = int(num_features)\n",
        "\n",
        "        random_indices = np.random.choice(num_features, size=num_features_for_split, replace=False)\n",
        "\n",
        "        for i, c in enumerate(X.T):\n",
        "          if (i in random_indices):\n",
        "            #print(\"col index: \" + str(i))\n",
        "            cur_cost, cur_cutoff = self.find_best_split(c, y)\n",
        "            if cur_cost <= min_cost:\n",
        "                min_cost = cur_cost\n",
        "                col = i\n",
        "                cutoff = cur_cutoff\n",
        "        #print(\"local best col: \" + str(col))\n",
        "        #print(\"local best cutoff: \" + str(cutoff))\n",
        "        #print(\"local best min_cost: \" + str(min_cost))\n",
        "        return col, cutoff, min_cost\n",
        "\n",
        "    def find_best_split(self, col, y):\n",
        "        min_cost = float(\"inf\")\n",
        "        cutoff = None\n",
        "\n",
        "        n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L = 0, 0, 0, 0\n",
        "        n_0_R = sum(y == 0)\n",
        "        n_plus_R = sum(y != 0)\n",
        "        y_plus_sum_R = sum(y)\n",
        "        y_plus_squared_sum_R = sum(y ** 2)\n",
        "\n",
        "        sorted_col_index = np.argsort(col)\n",
        "        sorted_y, sorted_col = y[sorted_col_index], col[sorted_col_index]\n",
        "\n",
        "        for i in range(len(col)):\n",
        "            if i == 0:\n",
        "              continue\n",
        "            else:\n",
        "              to_be_moved_y = sorted_y[i - 1]\n",
        "              if (to_be_moved_y == 0):\n",
        "                n_0_L += 1\n",
        "                n_0_R -= 1\n",
        "              else:\n",
        "                n_plus_L += 1\n",
        "                n_plus_R -= 1 \n",
        "                y_plus_sum_L += to_be_moved_y\n",
        "                y_plus_squared_sum_L += to_be_moved_y ** 2\n",
        "                y_plus_sum_R -= to_be_moved_y\n",
        "                y_plus_squared_sum_R -= to_be_moved_y ** 2             \n",
        "            if (n_0_L < 1 or n_plus_L < 2 or n_0_R < 1 or n_plus_R < 2):\n",
        "              continue\n",
        "            if sorted_col[i-1] == sorted_col[i]:\n",
        "              continue\n",
        "            cost_fn = self.get_cost(self.splitting_criteria)\n",
        "            current_cost = cost_fn(n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R)\n",
        "            #print(\"cut off value: \" + str(sorted_col[i]))\n",
        "            #print(\"current cost: \" + str(current_cost))\n",
        "            if current_cost <= min_cost:\n",
        "                min_cost = current_cost\n",
        "                cutoff = sorted_col[i]\n",
        "        return min_cost, cutoff\n",
        "\n",
        "    def predict_TOE(self, X):\n",
        "        results = np.array([0.0]*len(X))\n",
        "        for i, c in enumerate(X):\n",
        "            results[i] = self._get_prediction_TOE(c)\n",
        "        return results\n",
        "    \n",
        "    def _get_prediction_TOE(self, row):\n",
        "        cur_layer = self.trees\n",
        "        while cur_layer.get('cutoff'):\n",
        "            if row[cur_layer['col_index']] < cur_layer['cutoff']:\n",
        "                cur_layer = cur_layer['left']\n",
        "            else:\n",
        "                cur_layer = cur_layer['right']\n",
        "        else:\n",
        "            return cur_layer.get('val_TOE')\n",
        "\n",
        "    def predict_CME(self, X):\n",
        "        results = np.array([0.0]*len(X))\n",
        "        for i, c in enumerate(X):\n",
        "            results[i] = self._get_prediction_CME(c)\n",
        "        return results\n",
        "    \n",
        "    def _get_prediction_CME(self, row):\n",
        "        cur_layer = self.trees\n",
        "        while cur_layer.get('cutoff'):\n",
        "            if row[cur_layer['col_index']] < cur_layer['cutoff']:\n",
        "                cur_layer = cur_layer['left']\n",
        "            else:\n",
        "                cur_layer = cur_layer['right']\n",
        "        else:\n",
        "            return cur_layer.get('val_CME')\n",
        "\n",
        "    #  Non-self Functions\n",
        "    def basic_summary_statistics(self, y):\n",
        "      \"\"\"\n",
        "      Return n_0, n_+, y_+_mean, y_+_var\n",
        "      \"\"\"\n",
        "      y_0 = y[y == 0]\n",
        "      y_plus = y[y > 0]\n",
        "      n_0 = len(y_0)\n",
        "      n_plus = len(y_plus)\n",
        "      y_plus_mean = np.mean(y_plus)\n",
        "      y_plus_var = np.var(y_plus)\n",
        "\n",
        "      return n_0, n_plus, y_plus_mean, y_plus_var\n",
        "\n",
        "    def SSE_of_one_node(self, n_0, n_plus, y_plus_sum, y_plus_squared_sum):\n",
        "      \"\"\"\n",
        "      Return SSE (Sum of Squared Error) of a node\n",
        "      \"\"\"\n",
        "\n",
        "      return y_plus_squared_sum - y_plus_sum ** 2 / (n_0 + n_plus)\n",
        "\n",
        "\n",
        "    def get_SSE(self, n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R):\n",
        "      \"\"\"\n",
        "      Return SSE of a split\n",
        "      \"\"\"\n",
        "\n",
        "      return self.SSE_of_one_node(n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L) + self.SSE_of_one_node(n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R)\n",
        "\n",
        "\n",
        "    def get_t_squared(self, n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R):\n",
        "      \"\"\"\n",
        "      Return t^2 of a split\n",
        "      \"\"\"\n",
        "\n",
        "      y_plus_mean_L = y_plus_sum_L / n_plus_L\n",
        "      y_plus_mean_R = y_plus_sum_R / n_plus_R\n",
        "      sigma_squared_pool = (y_plus_squared_sum_L - (y_plus_sum_L ** 2 / n_plus_L) + y_plus_squared_sum_R - (y_plus_sum_R ** 2 / n_plus_R)) / (n_0_L + n_plus_L + n_0_R + n_plus_R - 4)\n",
        "      t_squared = -1.0 * (y_plus_mean_L - y_plus_mean_R) ** 2 / (sigma_squared_pool * (1 / n_0_L + 1 / n_plus_L + 1 / n_0_R + 1 / n_plus_R))\n",
        "      \n",
        "      return t_squared\n",
        "\n",
        "    def get_causal_cost(self, n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R):\n",
        "      \"\"\"\n",
        "      Return causal cost of a split\n",
        "      \"\"\"\n",
        "      y_plus_mean_L = y_plus_sum_L / n_plus_L\n",
        "      y_plus_mean_R = y_plus_sum_R / n_plus_R\n",
        "\n",
        "      causal_cost = -1.0 * ((n_0_L + n_plus_L) * y_plus_mean_L**2 + (n_0_R + n_plus_R) * y_plus_mean_R**2)\n",
        "\n",
        "      return causal_cost\n",
        "\n",
        "    def get_chi_squared_2(self, n_0_L, n_plus_L, y_plus_sum_L, y_plus_squared_sum_L, n_0_R, n_plus_R, y_plus_sum_R, y_plus_squared_sum_R):\n",
        "      \"\"\"\n",
        "      Return chi^2_2 of a split\n",
        "      \"\"\"\n",
        "      \n",
        "      y_plus_mean_L = y_plus_sum_L / n_plus_L\n",
        "      y_plus_mean_R = y_plus_sum_R / n_plus_R\n",
        "\n",
        "      sigma_squared_pool_plus = (y_plus_squared_sum_L - (y_plus_sum_L ** 2 / n_plus_L) + y_plus_squared_sum_R - (y_plus_sum_R ** 2 / n_plus_R)) / (n_plus_L + n_plus_R - 2)\n",
        "      positive_part_statistic = (y_plus_mean_L - y_plus_mean_R)**2 / ( sigma_squared_pool_plus * (1 / n_plus_L + 1 / n_plus_R))\n",
        "      \n",
        "      p_hat_L = n_0_L / (n_0_L + n_plus_L)\n",
        "      p_hat_R = n_0_R / (n_0_R + n_plus_R)\n",
        "      p_hat = (n_0_L + n_0_R) / (n_0_L + n_plus_L + n_0_R + n_plus_R)\n",
        "      zero_part_statistic = (p_hat_L - p_hat_R) ** 2 / (p_hat * (1 - p_hat) * (1 / (n_0_L + n_plus_L) + 1 / (n_0_R + n_plus_R)))\n",
        "      \n",
        "      return -1.0 * (positive_part_statistic + zero_part_statistic)\n",
        "\n",
        "    def get_cost(self, splitting_criteria):\n",
        "      if splitting_criteria == 'SSE':\n",
        "        return self.get_SSE\n",
        "      elif splitting_criteria == 't_squared':\n",
        "        return self.get_t_squared\n",
        "      elif splitting_criteria == 'causal':\n",
        "        return self.get_causal_cost\n",
        "      elif splitting_criteria == 'chi_squared_2':\n",
        "        return self.get_chi_squared_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIC1jQjHwhVt"
      },
      "source": [
        "## Random Forest Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08UItxu6wl9p"
      },
      "source": [
        "def SCRF(X_train, y_train, y_tilde_train, y_for_split, X_test, \n",
        "         estimation, splitting_criteria, splitter='sqrt', \n",
        "         max_depth=10, min_num_of_pos=2, min_num_of_zero=1, num_trees=100):\n",
        "  \n",
        "  SCRF_predicted = np.zeros(X_test.shape[0])\n",
        "\n",
        "  for tree_index in range(0, num_trees):\n",
        "    Semi_Continuous_Tree_ = Semi_Continuous_Tree(splitting_criteria, splitter, max_depth, min_num_of_pos, min_num_of_zero)\n",
        "    Semi_Continuous_Tree_.fit(X_train, y_train, y_tilde_train, y_for_split)\n",
        "    z_test_predicted = np.zeros(X_test.shape[0])\n",
        "    if (estimation == 'CME'):\n",
        "      SCRF_predicted += Semi_Continuous_Tree_.predict_CME(X_test)\n",
        "    elif (estimation == 'TOE'):\n",
        "      SCRF_predicted += Semi_Continuous_Tree_.predict_TOE(X_test)\n",
        "  \n",
        "  return SCRF_predicted / num_trees\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38sePtFoDtIS"
      },
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQXfCvDUgXNf"
      },
      "source": [
        "## RMSE Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npy5q1ORbNhb"
      },
      "source": [
        "def RMSE(z_hat, z):\n",
        "  return np.sqrt(sum((z_hat - z) ** 2) / len(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euCS9wM977eC"
      },
      "source": [
        "## RMSE Dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1quijT3Du-N"
      },
      "source": [
        "Synthetic_Data_RMSE_Dict = {\n",
        "  \"CMR_RMSE\": \n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"SSL_ST_RF_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"SSL_SSRF_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_SSE_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_SSE_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_t_squared_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_t_squared_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_causal_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_causal_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_chi_squared_2_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_chi_squared_2_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_SSE_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_SSE_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_t_squared_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_t_squared_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_causal_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_causal_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_chi_squared_2_CME_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "  \"y_tilde_chi_squared_2_TOE_RMSE\":\n",
        "  {\"f_1\": [],\"f_2\": [],\"f_3\": [],\"f_4\": [],\"f_5\": [],\"f_6\": [],\"f_7\": [],\"f_8\": []},\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnzay31N8C88"
      },
      "source": [
        "## Simulation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5PcdHoQdMNc"
      },
      "source": [
        "start = timeit.default_timer()\n",
        "trail = 100\n",
        "for t in range(trail):\n",
        "  print('Trail: ' + str(t))\n",
        "  for f in Synthetic_Data_RMSE_Dict['CMR_RMSE'].keys():\n",
        "    \n",
        "    print(f)\n",
        "    \n",
        "    np.random.seed(t)\n",
        "    X_train, z_train, y_train, y_tilde_train = training_data(f)\n",
        "    X_test, z_test, y_test, y_tilde_test = training_data(f)\n",
        "\n",
        "    print(\"CMR\")\n",
        "    CMR_Forest = RandomForestRegressor(n_estimators=100, max_features='sqrt', max_depth=Tree_Optimal_Depth[f])\n",
        "    CMR_Forest.fit(X_train[y_train>0], y_train[y_train>0])\n",
        "    z_test_hat_CMR_Forest = CMR_Forest.predict(X_test)\n",
        "    Synthetic_Data_RMSE_Dict[\"CMR_RMSE\"][f].append(RMSE(z_test_hat_CMR_Forest, z_test))\n",
        "\n",
        "    print(\"SSL ST RF\")\n",
        "    z_test_hat_SSL_ST_RF = semi_self_training_Forest(X_train, y_train, X_test, Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict[\"SSL_ST_RF_RMSE\"][f].append(RMSE(z_test_hat_SSL_ST_RF, z_test))\n",
        "\n",
        "    print(\"SSL SSRF\")\n",
        "    z_test_hat_SSL_SSRF = SSRF(X_train, y_train, X_test, 0.5, 'sqrt',  Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict[\"SSL_SSRF_RMSE\"][f].append(RMSE(z_test_hat_SSL_SSRF, z_test))\n",
        "    \n",
        "    print(\"y_SSE_CME_RMSE\")\n",
        "    z_test_hat_y_SSE_CME= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'CME', 'SSE', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_SSE_CME_RMSE'][f].append(RMSE(z_test_hat_y_SSE_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_SSE_TOE_RMSE\")\n",
        "    z_test_hat_y_SSE_TOE= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'TOE', 'SSE', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_SSE_TOE_RMSE'][f].append(RMSE(z_test_hat_y_SSE_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_t_squared_CME_RMSE\")\n",
        "    z_test_hat_y_t_squared_CME= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'CME', 't_squared', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_t_squared_CME_RMSE'][f].append(RMSE(z_test_hat_y_t_squared_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_t_squared_TOE_RMSE\")\n",
        "    z_test_hat_y_t_squared_TOE= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'TOE', 't_squared', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_t_squared_TOE_RMSE'][f].append(RMSE(z_test_hat_y_t_squared_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_causal_CME_RMSE\")\n",
        "    z_test_hat_y_causal_CME= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'CME', 'causal', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_causal_CME_RMSE'][f].append(RMSE(z_test_hat_y_causal_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_causal_TOE_RMSE\")\n",
        "    z_test_hat_y_causal_TOE= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'TOE', 'causal', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_causal_TOE_RMSE'][f].append(RMSE(z_test_hat_y_causal_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_chi_squared_2_CME_RMSE\")\n",
        "    z_test_hat_y_chi_squared_2_CME= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'CME', 'chi_squared_2', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_chi_squared_2_CME_RMSE'][f].append(RMSE(z_test_hat_y_chi_squared_2_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_chi_squared_2_TOE_RMSE\")\n",
        "    z_test_hat_y_chi_squared_2_TOE= SCRF(X_train, y_train, y_tilde_train, y_train, X_test, 'TOE', 'chi_squared_2', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_chi_squared_2_TOE_RMSE'][f].append(RMSE(z_test_hat_y_chi_squared_2_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_SSE_CME_RMSE\")\n",
        "    z_test_hat_y_tilde_SSE_CME= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'CME', 'SSE', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_SSE_CME_RMSE'][f].append(RMSE(z_test_hat_y_tilde_SSE_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_SSE_TOE_RMSE\")\n",
        "    z_test_hat_y_tilde_SSE_TOE= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'TOE', 'SSE', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_SSE_TOE_RMSE'][f].append(RMSE(z_test_hat_y_tilde_SSE_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_t_squared_CME_RMSE\")\n",
        "    z_test_hat_y_tilde_t_squared_CME= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'CME', 't_squared', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_t_squared_CME_RMSE'][f].append(RMSE(z_test_hat_y_tilde_t_squared_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_t_squared_TOE_RMSE\")\n",
        "    z_test_hat_y_tilde_t_squared_TOE= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'TOE', 't_squared', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_t_squared_TOE_RMSE'][f].append(RMSE(z_test_hat_y_tilde_t_squared_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_causal_CME_RMSE\")\n",
        "    z_test_hat_y_tilde_causal_CME= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'CME', 'causal', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_causal_CME_RMSE'][f].append(RMSE(z_test_hat_y_tilde_causal_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_causal_TOE_RMSE\")\n",
        "    z_test_hat_y_tilde_causal_TOE= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'TOE', 'causal', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_causal_TOE_RMSE'][f].append(RMSE(z_test_hat_y_tilde_causal_TOE, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_chi_squared_2_CME_RMSE\")\n",
        "    z_test_hat_y_tilde_chi_squared_2_CME= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'CME', 'chi_squared_2', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_chi_squared_2_CME_RMSE'][f].append(RMSE(z_test_hat_y_tilde_chi_squared_2_CME, z_test))\n",
        "\n",
        "\n",
        "    print(\"y_tilde_chi_squared_2_TOE_RMSE\")\n",
        "    z_test_hat_y_tilde_chi_squared_2_TOE= SCRF(X_train, y_train, y_tilde_train, y_tilde_train, X_test, 'TOE', 'chi_squared_2', 'sqrt', Tree_Optimal_Depth[f])\n",
        "    Synthetic_Data_RMSE_Dict['y_tilde_chi_squared_2_TOE_RMSE'][f].append(RMSE(z_test_hat_y_tilde_chi_squared_2_TOE, z_test))\n",
        "\n",
        "    if (t % 5 == 0):\n",
        "      pprint(Synthetic_Data_RMSE_Dict)\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvOMvvvR8Gxp"
      },
      "source": [
        "## Print Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqyOlIrnALFK"
      },
      "source": [
        "for model in Synthetic_Data_RMSE_Dict.keys():\n",
        "  print(model)\n",
        "  res = \"\"\n",
        "  for f in Synthetic_Data_RMSE_Dict['CMR_RMSE'].keys():\n",
        "      mean = np.mean(Synthetic_Data_RMSE_Dict[model][f])\n",
        "      mean = np.round(mean, 2)\n",
        "      std = np.std(Synthetic_Data_RMSE_Dict[model][f])\n",
        "      std = np.round(std, 2)\n",
        "      res += \"& $ \" + str(mean) + \" \\pm \" + str(std) + \" $\"\n",
        "  res += \" \\\\\\\\\"\n",
        "  print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B06Y47fhZ5GH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}